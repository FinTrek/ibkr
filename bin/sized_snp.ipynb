{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sized_snp.py\n",
    "\n",
    "\"\"\"Program to size nse options\n",
    "Date: 23-June-2019\n",
    "Ver: 1.0\n",
    "Time taken: 40 mins\n",
    "\"\"\"\n",
    "\n",
    "from z_helper import *\n",
    "util.startLoop()\n",
    "\n",
    "from chains_snp import *\n",
    "from ohlcs import *\n",
    "\n",
    "# # from json\n",
    "a = assign_var('common') + assign_var('snp')\n",
    "for v in a:\n",
    "    exec(v)\n",
    "\n",
    "def sized_snp(ib, df_chains, df_ohlcs):\n",
    "    '''Generates sized snp pickle\n",
    "    Args:\n",
    "        (ib) as connection object\n",
    "        (df_chains) from pickled / generated df_chains\n",
    "        (df_ohlcs) from pickled / generated df_ohlcs\n",
    "    Returns:\n",
    "        (df_optg) as group of sized df_opts. Also pickles'''\n",
    "\n",
    "    # log to size_chains.log\n",
    "    with open(logpath+'size_chains.log', 'w'):\n",
    "        pass # clear the run log\n",
    "    util.logToFile(logpath+'size_chains.log')\n",
    "\n",
    "    #... Remove chains not meeting put and call std filter\n",
    "\n",
    "    # get dte and remove those greater than maxdte\n",
    "    df_chains = df_chains.assign(dte=df_chains.expiry.apply(get_dte))                    \n",
    "    df_chains = df_chains[df_chains.dte <= maxdte]\n",
    "    \n",
    "    # replace dte with 1 for dte <= 0\n",
    "    df_chains.loc[df_chains.dte <=0,  'dte'] = 1\n",
    "\n",
    "    # generate std dataframe\n",
    "    df = df_ohlcs[['symbol', 'stDev']]  # lookup dataframe\n",
    "    df = df.assign(dte=df.groupby('symbol').cumcount()) # get the cumulative count for location as dte\n",
    "    df.set_index(['symbol', 'dte'])\n",
    "\n",
    "    df1 = df_chains[['symbol', 'dte']]  # data to be looked at\n",
    "    df2 = df1.drop_duplicates()  # remove duplicates\n",
    "\n",
    "    df_std = df2.set_index(['symbol', 'dte']).join(df.set_index(['symbol', 'dte']))\n",
    "\n",
    "    # join to get std in chains\n",
    "    df_chainstd = df_chains.set_index(['symbol', 'dte']).join(df_std).reset_index()\n",
    "\n",
    "    und_contracts = [Stock(symbol, exchange=exchange, currency='USD') for symbol in df_chainstd.symbol.unique()]\n",
    "#     with get_connected('snp', 'live') as ib:\n",
    "    und_quals = ib.qualifyContracts(*und_contracts)\n",
    "    tickers = ib.reqTickers(*und_quals)\n",
    "\n",
    "    uprice_dict = {u.contract.conId: u.marketPrice() for u in tickers}\n",
    "\n",
    "    df_chainstd = df_chainstd.assign(undPrice=df_chainstd.undId.astype('int').map(uprice_dict))\n",
    "\n",
    "    # make puts and calls dataframe with std filter\n",
    "    df_puts = df_chainstd[df_chainstd.strike < (df_chainstd.undPrice-(df_chainstd.stDev*putstdmult))]\n",
    "    df_puts = df_puts.assign(right = 'P')\n",
    "\n",
    "    df_calls = df_chainstd[df_chainstd.strike > (df_chainstd.undPrice+(df_chainstd.stDev*callstdmult))]\n",
    "    df_calls = df_calls.assign(right = 'C')\n",
    "\n",
    "    df_opt = pd.concat([df_puts, df_calls], sort=False).reset_index(drop=True)\n",
    "\n",
    "    # get lo52 and hi52\n",
    "    df_opt = df_opt.set_index('symbol').join(df_ohlcs.groupby('symbol')\n",
    "                                             .close.agg(['min', 'max'])\n",
    "                                             .rename(columns={'min': 'lo52', 'max': 'hi52'})).reset_index()\n",
    "\n",
    "    # make (df and dte) tuple for fallrise\n",
    "    tup4fr = [(df_ohlcs[df_ohlcs.symbol == s.symbol], s.dte) \n",
    "              for s in df_opt[['symbol', 'dte']].drop_duplicates().itertuples()]\n",
    "\n",
    "    # get the fallrise and put it into a dataframe\n",
    "    fr = [fallrise(*t) for t in tup4fr]\n",
    "    df_fr = pd.DataFrame(fr, columns=['symbol', 'dte', 'fall', 'rise' ])\n",
    "\n",
    "    # merge with df_opt\n",
    "    df_opt = pd.merge(df_opt, df_fr, on=['symbol', 'dte'])\n",
    "\n",
    "    # make reference strikes from fall_rise\n",
    "    df_opt = df_opt.assign(strikeRef = np.where(df_opt.right == 'P', \n",
    "                                                df_opt.undPrice-df_opt.fall, \n",
    "                                                df_opt.undPrice+df_opt.rise))\n",
    "\n",
    "    # get the strikes closest to the reference strikes\n",
    "    df_opt = df_opt.groupby(['symbol', 'dte']) \\\n",
    "                             .apply(lambda g: g.iloc[abs(g.strike-g.strikeRef) \\\n",
    "                                                     .argsort()[:nBand]])\n",
    "\n",
    "    df_opt = df_opt.set_index('symbol').reset_index()\n",
    "\n",
    "    # get the option contracts\n",
    "    opt_list = [Option(i.symbol, i.expiry, i.strike, i.right, 'SMART') for i in df_opt[['symbol', 'expiry', 'strike', 'right']].itertuples()]\n",
    "\n",
    "    # util.logToFile(logpath+'test.log') # prevents unknown contract errors in console\n",
    "\n",
    "    opt_contracts = []\n",
    "#     with get_connected('snp', 'live') as ib:\n",
    "    print(f\"\\nQualifying {len(opt_list)} option contracts ...\\n\")\n",
    "    opt_contracts = ib.qualifyContracts(*opt_list)\n",
    "\n",
    "    # integrate optId with df_opt and remove df_opt without optId\n",
    "    dfq = util.df(opt_contracts).iloc[:, 1:6]\n",
    "    dfq.columns=['optId', 'symbol', 'expiry', 'strike', 'right'] # rename columns\n",
    "    df_opt=df_opt.merge(dfq, on=['symbol', 'expiry', 'strike', 'right'], how='left')\n",
    "    df_opt = df_opt[~df_opt.optId.isnull()]\n",
    "    df_opt = df_opt.assign(optId=df_opt.optId.astype('int'))\n",
    "\n",
    "    # get the option prices\n",
    "#     with get_connected('snp', 'live') as ib:\n",
    "    ticker = ib.reqTickers(*opt_contracts)\n",
    "\n",
    "    df_prices = pd.DataFrame({t.contract.conId: {'bid':t.bid, 'ask':t.ask, 'close':t.close} for t in ticker}).T\n",
    "\n",
    "    # ...get margins\n",
    "\n",
    "    # prepare the lots\n",
    "    idlot_idx = df_opt[['optId', 'lot']].set_index('optId').to_dict('index')\n",
    "    idlot = {k: Order(action='SELL', orderType='MKT', totalQuantity=v['lot']/100, whatIf=True) for k, v in idlot_idx.items()}\n",
    "\n",
    "    co = [(c, idlot[c.conId]) for c in opt_contracts]\n",
    "\n",
    "    # co = co[:110]  # DATA LIMITER !!!\n",
    "    coblks = [co[i: i+blk] for i in range(0, len(co), blk)]\n",
    "\n",
    "    m = {} # empty dictionary to collect outputs of getMarginAsync\n",
    "\n",
    "#     with get_connected('snp', 'live') as ib:\n",
    "    async def coro(coblk):\n",
    "        with tqdm(total=len(coblk), file=sys.stdout, unit=' symexpiry') as tqm:\n",
    "            for c, o in coblk:\n",
    "                tqm.set_description(f\"IBKR margins for  {c.localSymbol.ljust(22)}\")\n",
    "                m.update(await getMarginAsync(ib, c, o))\n",
    "                tqm.update(1)\n",
    "            return m\n",
    "\n",
    "    # run co-routines to get the margins        \n",
    "    for coblk in coblks:\n",
    "#         with get_connected('snp', 'live') as ib:\n",
    "        asyncio.run(coro(coblk))\n",
    "\n",
    "    # put margins to df_opt\n",
    "    m_dict = {i: float(j.initMarginChange) for i, j in {k: v for k, v in m.items() if v}.items() if str(j) != 'nan'}\n",
    "\n",
    "    df_margin = pd.DataFrame.from_dict(m_dict, orient='index', columns=['margin'])\n",
    "\n",
    "    df_opt = df_opt.set_index('optId').join(df_prices).join(df_margin).reset_index()\n",
    "\n",
    "    df_opt = grp_opts(df_opt.assign(rom=df_opt.close/df_opt.margin*365/df_opt.dte*df_opt.lot))\n",
    "\n",
    "    df_opt.to_pickle(fspath+'sized_snp.pkl')\n",
    "    grp_opts(df_opt).to_excel(fspath+'sized_snp.xlsx', index=False, freeze_panes=(1,2))\n",
    "    \n",
    "    return df_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##### script ignored by jup2py\n",
    "\n",
    "from z_helper import *\n",
    "util.startLoop()\n",
    "\n",
    "from chains_snp import *\n",
    "from ohlcs import *\n",
    "\n",
    "# # from json\n",
    "a = assign_var('common') + assign_var('snp')\n",
    "for v in a:\n",
    "    exec(v)\n",
    "\n",
    "#... check for chains and ohlcs\n",
    "if path.isfile(fspath+'chains_snp.pkl'):\n",
    "    df_chains = pd.read_pickle(fspath+'snp_chains.pkl')\n",
    "else:\n",
    "    with get_connected('snp', 'live') as ib:\n",
    "        df_chains = get_chains(ib)\n",
    "\n",
    "if path.isfile(fspath+'ohlcs.pkl'):\n",
    "    df_ohlcs = pd.read_pickle(fspath+'ohlcs.pkl')\n",
    "\n",
    "else:\n",
    "    id_sym = df_chains.set_index('undId').symbol.to_dict()\n",
    "\n",
    "    with get_connected('snp', 'live') as ib:\n",
    "        df_ohlcs = ohlcs(ib, id_sym, fspath, logpath)\n",
    "\n",
    "    df_ohlcs.to_pickle(fspath+'ohlcs.pkl')\n",
    "\n",
    "# log to size_chains.log\n",
    "with open(logpath+'size_chains.log', 'w'):\n",
    "    pass # clear the run log\n",
    "util.logToFile(logpath+'size_chains.log')\n",
    "\n",
    "#... Remove chains not meeting put and call std filter\n",
    "\n",
    "# get dte and remove those greater than maxdte\n",
    "df_chains = df_chains.assign(dte=df_chains.expiry.apply(get_dte))                    \n",
    "df_chains = df_chains[df_chains.dte <= maxdte]\n",
    "\n",
    "# replace dte with 1 for dte <= 0\n",
    "df_chains.loc[df_chains.dte <=0,  'dte'] = 1\n",
    "\n",
    "# generate std dataframe\n",
    "df = df_ohlcs[['symbol', 'stDev']]  # lookup dataframe\n",
    "df = df.assign(dte=df.groupby('symbol').cumcount()) # get the cumulative count for location as dte\n",
    "df.set_index(['symbol', 'dte'])\n",
    "\n",
    "df1 = df_chains[['symbol', 'dte']]  # data to be looked at\n",
    "df2 = df1.drop_duplicates()  # remove duplicates\n",
    "\n",
    "df_std = df2.set_index(['symbol', 'dte']).join(df.set_index(['symbol', 'dte']))\n",
    "\n",
    "# join to get std in chains\n",
    "df_chainstd = df_chains.set_index(['symbol', 'dte']).join(df_std).reset_index()\n",
    "\n",
    "und_contracts = [Stock(symbol, exchange=exchange, currency='USD') for symbol in df_chainstd.symbol.unique()]\n",
    "with get_connected('snp', 'live') as ib:\n",
    "    und_quals = ib.qualifyContracts(*und_contracts)\n",
    "    tickers = ib.reqTickers(*und_quals)\n",
    "\n",
    "uprice_dict = {u.contract.conId: u.marketPrice() for u in tickers}\n",
    "\n",
    "df_chainstd = df_chainstd.assign(undPrice=df_chainstd.undId.astype('int').map(uprice_dict))\n",
    "\n",
    "# make puts and calls dataframe with std filter\n",
    "df_puts = df_chainstd[df_chainstd.strike < (df_chainstd.undPrice-(df_chainstd.stDev*putstdmult))]\n",
    "df_puts = df_puts.assign(right = 'P')\n",
    "\n",
    "df_calls = df_chainstd[df_chainstd.strike > (df_chainstd.undPrice+(df_chainstd.stDev*callstdmult))]\n",
    "df_calls = df_calls.assign(right = 'C')\n",
    "\n",
    "df_opt = pd.concat([df_puts, df_calls], sort=False).reset_index(drop=True)\n",
    "\n",
    "# get lo52 and hi52\n",
    "df_opt = df_opt.set_index('symbol').join(df_ohlcs.groupby('symbol')\n",
    "                                         .close.agg(['min', 'max'])\n",
    "                                         .rename(columns={'min': 'lo52', 'max': 'hi52'})).reset_index()\n",
    "\n",
    "# make (df and dte) tuple for fallrise\n",
    "tup4fr = [(df_ohlcs[df_ohlcs.symbol == s.symbol], s.dte) \n",
    "          for s in df_opt[['symbol', 'dte']].drop_duplicates().itertuples()]\n",
    "\n",
    "# get the fallrise and put it into a dataframe\n",
    "fr = [fallrise(*t) for t in tup4fr]\n",
    "df_fr = pd.DataFrame(fr, columns=['symbol', 'dte', 'fall', 'rise' ])\n",
    "\n",
    "# merge with df_opt\n",
    "df_opt = pd.merge(df_opt, df_fr, on=['symbol', 'dte'])\n",
    "\n",
    "# make reference strikes from fall_rise\n",
    "df_opt = df_opt.assign(strikeRef = np.where(df_opt.right == 'P', \n",
    "                                            df_opt.undPrice-df_opt.fall, \n",
    "                                            df_opt.undPrice+df_opt.rise))\n",
    "\n",
    "# get the strikes closest to the reference strikes\n",
    "df_opt = df_opt.groupby(['symbol', 'dte']) \\\n",
    "                         .apply(lambda g: g.iloc[abs(g.strike-g.strikeRef) \\\n",
    "                                                 .argsort()[:nBand]])\n",
    "\n",
    "df_opt = df_opt.set_index('symbol').reset_index()\n",
    "\n",
    "# get the option contracts\n",
    "opt_list = [Option(i.symbol, i.expiry, i.strike, i.right, 'SMART') for i in df_opt[['symbol', 'expiry', 'strike', 'right']].itertuples()]\n",
    "\n",
    "# util.logToFile(logpath+'test.log') # prevents unknown contract errors in console\n",
    "\n",
    "opt_contracts = []\n",
    "with get_connected('snp', 'live') as ib:\n",
    "    print(f\"\\nQualifying {len(opt_list)} option contracts ...\\n\")\n",
    "    opt_contracts = ib.qualifyContracts(*opt_list)\n",
    "\n",
    "# integrate optId with df_opt and remove df_opt without optId\n",
    "dfq = util.df(opt_contracts).iloc[:, 1:6]\n",
    "dfq.columns=['optId', 'symbol', 'expiry', 'strike', 'right'] # rename columns\n",
    "df_opt=df_opt.merge(dfq, on=['symbol', 'expiry', 'strike', 'right'], how='left')\n",
    "df_opt = df_opt[~df_opt.optId.isnull()]\n",
    "df_opt = df_opt.assign(optId=df_opt.optId.astype('int'))\n",
    "\n",
    "# get the option prices\n",
    "with get_connected('snp', 'live') as ib:\n",
    "    ticker = ib.reqTickers(*opt_contracts)\n",
    "\n",
    "df_prices = pd.DataFrame({t.contract.conId: {'bid':t.bid, 'ask':t.ask, 'close':t.close} for t in ticker}).T\n",
    "\n",
    "# ...get margins\n",
    "\n",
    "# prepare the lots\n",
    "idlot_idx = df_opt[['optId', 'lot']].set_index('optId').to_dict('index')\n",
    "idlot = {k: Order(action='SELL', orderType='MKT', totalQuantity=v['lot']/100, whatIf=True) for k, v in idlot_idx.items()}\n",
    "\n",
    "co = [(c, idlot[c.conId]) for c in opt_contracts]\n",
    "\n",
    "# co = co[:110]  # DATA LIMITER !!!\n",
    "coblks = [co[i: i+blk] for i in range(0, len(co), blk)]\n",
    "\n",
    "m = {} # empty dictionary to collect outputs of getMarginAsync\n",
    "\n",
    "with get_connected('snp', 'live') as ib:\n",
    "    async def coro(coblk):\n",
    "        with tqdm(total=len(coblk), file=sys.stdout, unit=' symexpiry') as tqm:\n",
    "            for c, o in coblk:\n",
    "                tqm.set_description(f\"IBKR margins for  {c.localSymbol.ljust(22)}\")\n",
    "                m.update(await getMarginAsync(ib, c, o))\n",
    "                tqm.update(1)\n",
    "            return m\n",
    "\n",
    "# run co-routines to get the margins        \n",
    "for coblk in coblks:\n",
    "    with get_connected('snp', 'live') as ib:\n",
    "        asyncio.run(coro(coblk))\n",
    "\n",
    "# put margins to df_opt\n",
    "m_dict = {i: float(j.initMarginChange) for i, j in {k: v for k, v in m.items() if v}.items() if str(j) != 'nan'}\n",
    "\n",
    "df_margin = pd.DataFrame.from_dict(m_dict, orient='index', columns=['margin'])\n",
    "\n",
    "df_opt = df_opt.set_index('optId').join(df_prices).join(df_margin).reset_index()\n",
    "\n",
    "df_opt = grp_opts(df_opt.assign(rom=df_opt.close/df_opt.margin*365/df_opt.dte*df_opt.lot))\n",
    "\n",
    "df_opt.to_pickle(fspath+'sized_snp.pkl')\n",
    "grp_opts(df_opt).to_excel(fspath+'sized_snp.xlsx', index=False, freeze_panes=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qualifying 3680 option contracts ...\n",
      "\n",
      "IBKR margins for  ABT   190809P00079000 : 100%|████████████████████████████████| 50/50 [00:19<00:00,  2.59 symexpiry/s]\n",
      "IBKR margins for  ADBE  190802P00267500 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.04 symexpiry/s]\n",
      "IBKR margins for  AMGN  190726C00180000 : 100%|████████████████████████████████| 50/50 [00:51<00:00,  1.03 symexpiry/s]\n",
      "IBKR margins for  AXP   190809C00141000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.04 symexpiry/s]\n",
      "IBKR margins for  BAC   190816P00023500 : 100%|████████████████████████████████| 50/50 [00:51<00:00,  1.03 symexpiry/s]\n",
      "IBKR margins for  BK    190830P00037500 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.02s/ symexpiry]\n",
      "IBKR margins for  BMY   190726P00039000 : 100%|████████████████████████████████| 50/50 [00:54<00:00,  1.14 symexpiry/s]\n",
      "IBKR margins for  C     190816C00090000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.02 symexpiry/s]\n",
      "IBKR margins for  CHTR  190920P00280000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.00 symexpiry/s]\n",
      "IBKR margins for  CMCSA 190906C00051500 : 100%|████████████████████████████████| 50/50 [00:47<00:00,  1.02 symexpiry/s]\n",
      "IBKR margins for  COST  190802C00305000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.06 symexpiry/s]\n",
      "IBKR margins for  CVS   190920C00070000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.08s/ symexpiry]\n",
      "IBKR margins for  DIS   190809P00128000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.11 symexpiry/s]\n",
      "IBKR margins for  F     190802C00011000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.01s/ symexpiry]\n",
      "IBKR margins for  FB    190920C00250000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.02 symexpiry/s]\n",
      "IBKR margins for  GE    190726P00009000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.01 symexpiry/s]\n",
      "IBKR margins for  GILD  190920P00050000 : 100%|████████████████████████████████| 50/50 [00:47<00:00,  1.06 symexpiry/s]\n",
      "IBKR margins for  GOOG  190920C01380000 : 100%|████████████████████████████████| 50/50 [00:51<00:00,  1.19s/ symexpiry]\n",
      "IBKR margins for  HD    190920P00170000 : 100%|████████████████████████████████| 50/50 [00:47<00:00,  1.05 symexpiry/s]\n",
      "IBKR margins for  IBM   190920C00180000 : 100%|████████████████████████████████| 50/50 [00:47<00:00,  1.12 symexpiry/s]\n",
      "IBKR margins for  JPM   190726P00110000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.10 symexpiry/s]\n",
      "IBKR margins for  KMI   190816C00024000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.03 symexpiry/s]\n",
      "IBKR margins for  KO    190920P00047000 : 100%|████████████████████████████████| 50/50 [00:52<00:00,  1.00s/ symexpiry]\n",
      "IBKR margins for  LOW   190726C00109000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.04 symexpiry/s]\n",
      "IBKR margins for  MCD   190802P00197500 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.95 symexpiry/s]\n",
      "IBKR margins for  MDLZ  190920P00046000 : 100%|████████████████████████████████| 50/50 [00:37<00:00,  1.04 symexpiry/s]\n",
      "IBKR margins for  MMM   190726P00167500 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.01 symexpiry/s]\n",
      "IBKR margins for  MRK   190802P00073000 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.04 symexpiry/s]\n",
      "IBKR margins for  MSFT  190802P00126000 : 100%|████████████████████████████████| 50/50 [00:52<00:00,  1.06 symexpiry/s]\n",
      "IBKR margins for  NKE   190809P00075000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.26s/ symexpiry]\n",
      "IBKR margins for  PEP   190809C00139000 : 100%|████████████████████████████████| 50/50 [00:53<00:00,  1.10s/ symexpiry]\n",
      "IBKR margins for  PG    190802C00124000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.08 symexpiry/s]\n",
      "IBKR margins for  PYPL  190726P00105000 : 100%|████████████████████████████████| 50/50 [00:46<00:00,  1.12 symexpiry/s]\n",
      "IBKR margins for  RTN   190816P00150000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.05s/ symexpiry]\n",
      "IBKR margins for  T     190726C00035500 : 100%|████████████████████████████████| 50/50 [01:00<00:00,  1.00s/ symexpiry]\n",
      "IBKR margins for  TXN   190726C00132000 : 100%|████████████████████████████████| 50/50 [00:47<00:00,  1.15 symexpiry/s]\n",
      "IBKR margins for  UNH   190920P00185000 : 100%|████████████████████████████████| 50/50 [00:50<00:00,  1.05 symexpiry/s]\n",
      "IBKR margins for  UPS   190830P00090000 : 100%|████████████████████████████████| 50/50 [00:49<00:00,  1.02s/ symexpiry]\n",
      "IBKR margins for  V     190726C00192500 : 100%|████████████████████████████████| 50/50 [00:48<00:00,  1.21 symexpiry/s]\n",
      "IBKR margins for  WBA   190726C00059000 : 100%|████████████████████████████████| 50/50 [00:52<00:00,  1.26s/ symexpiry]\n",
      "IBKR margins for  WMT   190920C00135000 : 100%|████████████████████████████████| 50/50 [00:51<00:00,  1.11s/ symexpiry]\n",
      "IBKR margins for  XOM   190920C00095000 : 100%|████████████████████████████████| 23/23 [00:24<00:00,  1.01 symexpiry/s]\n"
     ]
    }
   ],
   "source": [
    "# test sized_snp function\n",
    "\n",
    "df_chains = pd.read_pickle(fspath+'chains_snp.pkl')\n",
    "df_ohlcs = pd.read_pickle(fspath+'ohlcs.pkl')\n",
    "\n",
    "with get_connected('snp', 'live') as ib:\n",
    "    df_sized = sized_snp(ib=ib, df_chains=df_chains, df_ohlcs=df_ohlcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
