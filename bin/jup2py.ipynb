{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# program to convert functions in cells to indivdiual .py files\n",
    "# the first line of the cell containing function to be converted should have\n",
    "# filename.py\n",
    "\n",
    "import json\n",
    "from os import listdir, path\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "fs = listdir() # get the filenames\n",
    "\n",
    "# make an exclusion list\n",
    "magics = %lsmagic\n",
    "magic_words = [s for s in str(magics).split(' ') if s not in '' if '%' in s[0]]\n",
    "magic_words = [s for s in magic_words if s != '%']\n",
    "\n",
    "exclude = ['util.startLoop()', 'import nest_asyncio', 'nest_asyncio.apply()'] + magic_words\n",
    "\n",
    "# searchfor = ['helper.ipynb', 'nse_func.ipynb', 'snp_func.ipynb', 'nse_main.ipynb', 'snp_main.ipynb']  # list of files to be converted into\n",
    "# ipfilelist = [f for f in fs if any(word in f for word in searchfor)]\n",
    "\n",
    "# remove unwanted file extensions, prepends and this file itself!\n",
    "ipfilelist = [f for f in fs if f[-5:] == 'ipynb' if f[:1] not in ['_'] if f[:-6] not in 'jup2py']\n",
    "\n",
    "for file in ipfilelist:\n",
    "    code_cells = []  #initialize code_cells\n",
    "    with open(file) as datafile:\n",
    "        code = json.load(datafile)\n",
    "        code_cells.append([cell['source'] for cell in code['cells'] if cell['cell_type'] == 'code'])\n",
    "        codes = [cell for cells in code_cells for cell in cells if cell]\n",
    "        code_dict = {pycode[0][2:-1]:pycode for pycode in codes if pycode[0][-4:] == '.py\\n'}\n",
    "        with open(file[:-6]+'.py', 'w') as f:\n",
    "            for k, v in code_dict.items():\n",
    "                for line in v:\n",
    "                    if not any(word in line for word in exclude):\n",
    "                        f.write(line)        \n",
    "                f.write('\\n\\n#_____________________________________\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ipfilelist[3]) as dfile:\n",
    "    code=json.load(dfile)\n",
    "    code_cells.append([cell['source'] for cell in code['cells'] if cell['cell_type'] == 'code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%%time\\n',\n",
       " '# first script\\n',\n",
       " 'from z_helper import *\\n',\n",
       " 'util.startLoop()\\n',\n",
       " '\\n',\n",
       " 'from chains_nse import *\\n',\n",
       " 'from ohlcs import *\\n',\n",
       " '\\n',\n",
       " '# # from json\\n',\n",
       " \"a = assign_var('common') + assign_var('nse')\\n\",\n",
       " 'for v in a:\\n',\n",
       " '    exec(v)\\n',\n",
       " '\\n',\n",
       " '#... check for chains and ohlcs\\n',\n",
       " \"if path.isfile(fspath+'chains_nse.pkl'):\\n\",\n",
       " \"    df_chains = pd.read_pickle(fspath+'chains_nse.pkl')\\n\",\n",
       " 'else:\\n',\n",
       " \"    with get_connected('nse', 'live') as ib:\\n\",\n",
       " '        df_chains = get_chains(ib)\\n',\n",
       " '\\n',\n",
       " \"if path.isfile(fspath+'ohlcs.pkl'):\\n\",\n",
       " \"    df_ohlcs = pd.read_pickle(fspath+'ohlcs.pkl')\\n\",\n",
       " '\\n',\n",
       " 'else:\\n',\n",
       " \"    id_sym = df_chains.set_index('undId').symbol.to_dict()\\n\",\n",
       " '\\n',\n",
       " \"    with get_connected('nse', 'live') as ib:\\n\",\n",
       " '        df_ohlcs = ohlcs(ib, id_sym, fspath, logpath)\\n',\n",
       " '\\n',\n",
       " \"    df_ohlcs.to_pickle(fspath+'ohlcs.pkl')\\n\",\n",
       " '\\n',\n",
       " \"# ib = get_connected('nse', 'live')\\n\",\n",
       " \"# df_chains = pd.read_pickle(fspath+'chains_nse.pkl')\\n\",\n",
       " \"# df_ohlcs = pd.read_pickle(fspath+'ohlcs.pkl')\\n\",\n",
       " '\\n',\n",
       " '# log to size_chains.log\\n',\n",
       " \"with open(logpath+'size_chains.log', 'w'):\\n\",\n",
       " '    pass # clear the run log\\n',\n",
       " \"util.logToFile(logpath+'size_chains.log')\\n\",\n",
       " '\\n',\n",
       " '#... Remove chains not meeting put and call std filter\\n',\n",
       " '\\n',\n",
       " '# get dte and remove those greater than maxdte\\n',\n",
       " 'df_chains = df_chains.assign(dte=df_chains.expiry.apply(get_dte))                    \\n',\n",
       " 'df_chains = df_chains[df_chains.dte <= maxdte]\\n',\n",
       " '\\n',\n",
       " '# generate std dataframe\\n',\n",
       " \"df = df_ohlcs[['symbol', 'stDev']]  # lookup dataframe\\n\",\n",
       " \"df = df.assign(dte=df.groupby('symbol').cumcount()) # get the cumulative count for location as dte\\n\",\n",
       " \"df.set_index(['symbol', 'dte'])\\n\",\n",
       " '\\n',\n",
       " \"df1 = df_chains[['symbol', 'dte']]  # data to be looked at\\n\",\n",
       " 'df2 = df1.drop_duplicates()  # remove duplicates\\n',\n",
       " '\\n',\n",
       " \"df_std = df2.set_index(['symbol', 'dte']).join(df.set_index(['symbol', 'dte']))\\n\",\n",
       " '\\n',\n",
       " '# join to get std in chains\\n',\n",
       " \"df_chainstd = df_chains.set_index(['symbol', 'dte']).join(df_std).reset_index()\\n\",\n",
       " '\\n',\n",
       " '# make puts and calls dataframe with std filter\\n',\n",
       " 'df_puts = df_chainstd[df_chainstd.strike < (df_chainstd.undPrice-(df_chainstd.stDev*putstdmult))]\\n',\n",
       " \"df_puts = df_puts.assign(right = 'P')\\n\",\n",
       " '\\n',\n",
       " 'df_calls = df_chainstd[df_chainstd.strike > (df_chainstd.undPrice+(df_chainstd.stDev*callstdmult))]\\n',\n",
       " \"df_calls = df_calls.assign(right = 'C')\\n\",\n",
       " '\\n',\n",
       " 'df_opt = pd.concat([df_puts, df_calls], sort=False).reset_index(drop=True)\\n',\n",
       " '\\n',\n",
       " '# get lo52 and hi52\\n',\n",
       " \"df_opt = df_opt.set_index('symbol').join(df_ohlcs.groupby('symbol')\\n\",\n",
       " \"                                         .close.agg(['min', 'max'])\\n\",\n",
       " \"                                         .rename(columns={'min': 'lo52', 'max': 'hi52'})).reset_index()\\n\",\n",
       " '\\n',\n",
       " '# make (df and dte) tuple for fallrise\\n',\n",
       " 'tup4fr = [(df_ohlcs[df_ohlcs.symbol == s.symbol], s.dte) \\n',\n",
       " \"          for s in df_opt[['symbol', 'dte']].drop_duplicates().itertuples()]\\n\",\n",
       " '\\n',\n",
       " '# get the fallrise and put it into a dataframe\\n',\n",
       " 'fr = [fallrise(*t) for t in tup4fr]\\n',\n",
       " \"df_fr = pd.DataFrame(fr, columns=['symbol', 'dte', 'fall', 'rise' ])\\n\",\n",
       " '\\n',\n",
       " '# merge with df_opt\\n',\n",
       " \"df_opt = pd.merge(df_opt, df_fr, on=['symbol', 'dte'])\\n\",\n",
       " '\\n',\n",
       " '# make reference strikes from fall_rise\\n',\n",
       " \"df_opt = df_opt.assign(strikeRef = np.where(df_opt.right == 'P', \\n\",\n",
       " '                                            df_opt.undPrice-df_opt.fall, \\n',\n",
       " '                                            df_opt.undPrice+df_opt.rise))\\n',\n",
       " '\\n',\n",
       " '# get the strikes closest to the reference strikes\\n',\n",
       " \"df_opt = df_opt.groupby(['symbol', 'dte']) \\\\\\n\",\n",
       " '                         .apply(lambda g: g.iloc[abs(g.strike-g.strikeRef) \\\\\\n',\n",
       " '                                                 .argsort()[:nBand]])\\n',\n",
       " '\\n',\n",
       " \"df_opt = df_opt.set_index('symbol').reset_index()\\n\",\n",
       " '\\n',\n",
       " '# get the option contracts\\n',\n",
       " \"opt_list = [Option(i.symbol, i.expiry, i.strike, i.right, 'NSE') for i in df_opt[['symbol', 'expiry', 'strike', 'right']].itertuples()]\\n\",\n",
       " '\\n',\n",
       " \"util.logToFile(logpath+'test.log') # prevents unknown contract errors in console\\n\",\n",
       " '\\n',\n",
       " 'opt_contracts = []\\n',\n",
       " \"with get_connected('nse', 'live') as ib:\\n\",\n",
       " '    print(\"Qualifying option contracts ...(2 to 8 mins)\")\\n',\n",
       " '    opt_contracts = ib.qualifyContracts(*opt_list)\\n',\n",
       " '\\n',\n",
       " '# integrate optId with df_opt and remove df_opt without optId\\n',\n",
       " 'dfq = util.df(opt_contracts).iloc[:, 1:6]\\n',\n",
       " \"dfq.columns=['optId', 'symbol', 'expiry', 'strike', 'right'] # rename columns\\n\",\n",
       " \"df_opt=df_opt.merge(dfq, on=['symbol', 'expiry', 'strike', 'right'], how='left')\\n\",\n",
       " 'df_opt = df_opt[~df_opt.optId.isnull()]\\n',\n",
       " \"df_opt = df_opt.assign(optId=df_opt.optId.astype('int'))\\n\",\n",
       " '\\n',\n",
       " '# get the option prices\\n',\n",
       " \"with get_connected('nse', 'live') as ib:\\n\",\n",
       " '    ticker = ib.reqTickers(*opt_contracts)\\n',\n",
       " '\\n',\n",
       " \"df_prices = pd.DataFrame({t.contract.conId: {'bid':t.bid, 'ask':t.ask, 'close':t.close} for t in ticker}).T\\n\",\n",
       " '\\n',\n",
       " '# ...get margins\\n',\n",
       " '\\n',\n",
       " '# prepare the lots\\n',\n",
       " \"idlot_idx = df_opt[['optId', 'lot']].set_index('optId').to_dict('index')\\n\",\n",
       " \"idlot = {k: Order(action='SELL', orderType='MKT', totalQuantity=v['lot'], whatIf=True) for k, v in idlot_idx.items()}\\n\",\n",
       " '\\n',\n",
       " 'co = [(c, idlot[c.conId]) for c in opt_contracts]\\n',\n",
       " '\\n',\n",
       " '# co = co[:110]  # DATA LIMITER !!!\\n',\n",
       " 'coblks = [co[i: i+blk] for i in range(0, len(co), blk)]\\n',\n",
       " '\\n',\n",
       " 'm = {} # empty dictionary to collect outputs of getMarginAsync\\n',\n",
       " \"with get_connected('nse', 'live') as ib:\\n\",\n",
       " '    async def coro(coblk):\\n',\n",
       " \"        with tqdm(total=len(coblk), file=sys.stdout, unit=' symexpiry') as tqm:\\n\",\n",
       " '            for c, o in coblk:\\n',\n",
       " '                tqm.set_description(f\"IBKR margins for  {c.localSymbol.ljust(22)}\")\\n',\n",
       " '                m.update(await getMarginAsync(ib, c, o))\\n',\n",
       " '                tqm.update(1)\\n',\n",
       " '            return m\\n',\n",
       " '\\n',\n",
       " '# run co-routines to get the margins        \\n',\n",
       " 'for coblk in coblks:\\n',\n",
       " \"    with get_connected('nse', 'live') as ib:\\n\",\n",
       " '        asyncio.run(coro(coblk))\\n',\n",
       " '\\n',\n",
       " '# put margins to df_opt\\n',\n",
       " \"m_dict = {i: float(j.initMarginChange) for i, j in {k: v for k, v in m.items() if v}.items() if str(j) != 'nan'}\\n\",\n",
       " '\\n',\n",
       " \"df_margin = pd.DataFrame.from_dict(m_dict, orient='index', columns=['margin'])\\n\",\n",
       " '\\n',\n",
       " \"df_opt = df_opt.set_index('optId').join(df_prices).join(df_margin).reset_index()\\n\",\n",
       " '\\n',\n",
       " 'df_opt = grp_opts(df_opt.assign(rom=df_opt.close/df_opt.margin*365/df_opt.dte*df_opt.lot))\\n',\n",
       " '\\n',\n",
       " \"df_opt.to_pickle(fspath+'sized.pkl')\\n\",\n",
       " \"grp_opts(df_opt).to_excel(fspath+'sized_nse.xlsx', index=False, freeze_panes=(1,2))\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code['cells'][0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
